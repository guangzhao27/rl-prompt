import torch
from torch.utils.data import Dataset, DataLoader
from typing import Optional, Callable, Dict, Any, Union, List, Tuple
import torch.nn.functional as F
import random
from rlprompt.modules import BaseModule
from rlprompt.utils import utils
from .trainer_utils import get_default_train_op, set_random_seed, find_pareto_front, calculate_dominating_volume, evaluate_model_dominate_volume

from rlprompt.trainers import Trainer
    


def reward_diff(
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
    ):
    pi_logratios = policy_chosen_logps - policy_rejected_logps
    ref_logratios = reference_chosen_logps - reference_rejected_logps

    logits = pi_logratios - ref_logratios
    return logits


def preference_loss(policy_chosen_logps: torch.FloatTensor,
                    policy_rejected_logps: torch.FloatTensor,
                    reference_chosen_logps: torch.FloatTensor,
                    reference_rejected_logps: torch.FloatTensor,
                    dpo_loss_config) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:
    """Compute the DPO loss for a batch of policy and reference model log probabilities.

    Args:
        policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)
        policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)
        reference_chosen_logps: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)
        reference_rejected_logps: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)
        beta: Temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -> 0.
        label_smoothing: conservativeness for DPO loss, which assumes that preferences are noisy (flipped with probability label_smoothing)
        ipo: If True, use the IPO loss instead of the DPO loss.
        reference_free: If True, we ignore the _provided_ reference model and implicitly use a reference model that assigns equal probability to all responses.

    Returns:
        A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).
        The losses tensor contains the DPO loss for each example in the batch.
        The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.
    """
    name = dpo_loss_config.name
    beta = dpo_loss_config.beta
    # reference_free = dpo_loss_config.reference_free
    label_smoothing = dpo_loss_config.label_smoothing

    logits = reward_diff(policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps)
    # also known as h_{\pi_\theta}^{y_w,y_l}

    if name == "ipo":
        losses = (logits - 1/(2 * beta)) ** 2  # Eq. 17 of https://arxiv.org/pdf/2310.12036v2.pdf
    elif name == "dpo":
        # Eq. 3 https://ericmitchell.ai/cdpo.pdf; label_smoothing=0 gives original DPO (Eq. 7 of https://arxiv.org/pdf/2305.18290.pdf)
        losses = -F.logsigmoid(beta * logits) * (1 - label_smoothing) - F.logsigmoid(-beta * logits) * label_smoothing
    else:
        raise ValueError(f'unknown loss {name}')

    chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps).detach()
    rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps).detach()

    return losses, chosen_rewards, rejected_rewards


class DPO_Trainer(Trainer):
    def __init__(
        self,
        module: BaseModule,
        # Train params
        train_dataset: Optional[Dataset],
        train_batch_size: int,
        train_shuffle: bool,
        train_drop_last: bool,
        num_train_epochs: int,
        max_train_steps: int,
        # Eval params
        do_eval: bool,
        eval_dataset: Optional[Dataset],
        eval_batch_size: int,
        eval_steps: int,
        # Save params
        do_save: bool,
        save_dir: str,
        save_steps: int,
        # Optimizer params
        learning_rate: float,
        gradient_clip: bool,
        gradient_clip_norm: float,
        # Checkpoint params
        checkpoint_path: Optional[str],
        # Random seed
        random_seed: Optional[int],
        # Wandb reporting
        report_to_wandb: bool,
        project_name: Optional[str],
        run_name: Optional[str],
        dpo_loss_config: Dict[str, Any],
    ):
        super().__init__(
            module,
            train_dataset,
            train_batch_size,
            train_shuffle,
            train_drop_last,
            num_train_epochs,
            max_train_steps,
            do_eval,
            eval_dataset,
            eval_batch_size,
            eval_steps,
            do_save,
            save_dir,
            save_steps,
            learning_rate,
            gradient_clip,
            gradient_clip_norm,
            checkpoint_path,
            random_seed,
            report_to_wandb,
            project_name,
            run_name, 
        )
        self.dpo_loss_config = dpo_loss_config
        
    def _train_step(self, step: int, batch: Dict[str, Any]) -> Dict[str, Any]:
        policy_model = self.module.train()
        policy_model._pre_steps(step)  
        ## it updates reference model, while with learning rate=0.0, it doesn't learn anything
        # /hpcgpfs01/scratch/gzhao/rl-prompt/rlprompt/modules/sql_module.py _pre_steps()
        
        
        
        
        
        (policy_logits, ref_logits, output_tokens, output_ids, sequence_lengths) = \
            policy_model._decode_sampling(batch=batch)  ## TODO: where should these samples being sampled from??
            
        raw_rewards, rewards_log = \
            policy_model.compute_rewards(batch=batch, 
                                output_tokens=output_tokens,
                                mode="train")
        
        reward_matrix = policy_model._reward_shaping_func(raw_rewards)
        sample_num = reward_matrix.shape[0]
        token_lenght = output_ids.shape[1]
        
        policy_all_logps = policy_logits[
            torch.arange(sample_num).unsqueeze(1), 
            torch.arange(token_lenght), 
            output_ids
        ].sum(axis=1)
        
        # ref_model = copy.deepcopy(policy_model).eval()
        # ref_model._pre_steps(step)
        # outputs_ = ref_model.teacher_forcing(**batch)  ## what is ** batch, check if ref_model update, follow the debuuging process s
        
        # ref_logits = outputs_['sample_logits']
        if self.dpo_loss_config.name == 'reinforce':
            log_prob = F.log_softmax(policy_logits, dim=-1)
            policy_all_logps = log_prob[
                torch.arange(sample_num).unsqueeze(1), 
                torch.arange(token_lenght), 
                output_ids
            ].sum(axis=1)
            losses = -policy_all_logps*raw_rewards
        else:
            ref_all_logps = ref_logits[
                torch.arange(sample_num).unsqueeze(1), 
                torch.arange(token_lenght), 
                output_ids
            ].sum(axis=1)
            
            
            chosen_idx_list = torch.zeros(sample_num // 2, dtype=torch.long)
            rejected_idx_list = torch.zeros(sample_num // 2, dtype=torch.long)
            for i in range(sample_num // 2):
                if reward_matrix[2*i] > reward_matrix[2*i + 1]:
                    chosen_idx_list[i] = 2 * i
                    rejected_idx_list[i] = 2 * i + 1
                else:
                    chosen_idx_list[i] = 2 * i + 1
                    rejected_idx_list[i] = 2 * i## make it random?
            
            
            
            policy_chosen_logps = policy_all_logps[chosen_idx_list]
            policy_rejected_logps = policy_all_logps[rejected_idx_list]
            with torch.no_grad():
                reference_chosen_logps = ref_all_logps[chosen_idx_list]
                reference_rejected_logps = ref_all_logps[rejected_idx_list]
            
            if self.dpo_loss_config.name == 'dpo':
                    loss_kwargs = {'beta': self.dpo_loss_config.beta, 'reference_free': self.dpo_loss_config.reference_free, 'label_smoothing': self.dpo_loss_config.label_smoothing, 'ipo': False}
            elif self.dpo_loss_config.name == 'ipo':
                loss_kwargs = {'beta': self.dpo_loss_config.beta, 'ipo': True}
            else:
                raise ValueError(f'unknown loss {self.dpo_loss_config.name}')

            losses, chosen_rewards, rejected_rewards = preference_loss(
                policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps, **loss_kwargs)

            reward_accuracies = (chosen_rewards > rejected_rewards).float()
        
        loss = losses.mean()
        loss.backward()
        
        self.optimizer.step()
        self.optimizer.zero_grad()
        optimizer_state = self.optimizer.state_dict()
        
        # optimizer_state = self.train_op()
        
        batch_log = rewards_log  ## add more recorded metrics in this log
        
        utils.add_prefix_to_dict_keys_inplace(
                rewards_log, prefix="SQL_ON/rewards/")
        
        batch_log = rewards_log
        batch_log['dpo_loss'] = loss

        return batch_log, optimizer_state
       
       
class DPO_O2_Trainer(Trainer):
    def __init__(
        self,
        module: BaseModule,
        # Train params
        train_dataset: Optional[Dataset],
        train_batch_size: int,
        train_shuffle: bool,
        train_drop_last: bool,
        num_train_epochs: int,
        max_train_steps: int,
        # Eval params
        do_eval: bool,
        eval_dataset: Optional[Dataset],
        eval_batch_size: int,
        eval_steps: int,
        # Save params
        do_save: bool,
        save_dir: str,
        save_steps: int,
        # Optimizer params
        learning_rate: float,
        gradient_clip: bool,
        gradient_clip_norm: float,
        # Checkpoint params
        checkpoint_path: Optional[str],
        # Random seed
        random_seed: Optional[int],
        # Wandb reporting
        report_to_wandb: bool,
        project_name: Optional[str],
        run_name: Optional[str],
        dpo_loss_config: Dict[str, Any],
    ):
        super().__init__(
            module,
            train_dataset,
            train_batch_size,
            train_shuffle,
            train_drop_last,
            num_train_epochs,
            max_train_steps,
            do_eval,
            eval_dataset,
            eval_batch_size,
            eval_steps,
            do_save,
            save_dir,
            save_steps,
            learning_rate,
            gradient_clip,
            gradient_clip_norm,
            checkpoint_path,
            random_seed,
            report_to_wandb,
            project_name,
            run_name, 
        )
        self.dpo_loss_config = dpo_loss_config
        
    def loss_func(self, step, batch, config):
        policy_model = self.module.train()
        policy_model._pre_steps(step)  
        
        (policy_logits, ref_logits, output_tokens, output_ids, sequence_lengths) = \
            policy_model._decode_sampling(batch=batch) # for multiobjective batch size should be large
        
        sample_num = policy_logits.shape[0]
        token_lenght = policy_logits.shape[1]
        
        policy_all_logps = policy_logits[
            torch.arange(sample_num).unsqueeze(1), 
            torch.arange(token_lenght), 
            output_ids
        ].sum(axis=1)
        
        ref_all_logps = ref_logits[
            torch.arange(sample_num).unsqueeze(1), 
            torch.arange(token_lenght), 
            output_ids
        ].sum(axis=1)
        return policy_all_logps, ref_all_logps
        

    def _train_step(self, step: int, batch: Dict[str, Any]) -> Dict[str, Any]:
        
        loss, rewards_log = self.dpo_loss_calculation(step, batch, self.dpo_loss_config)
        
        loss.backward()#0.4288172721862793
        
        self.optimizer.step()
        self.optimizer.zero_grad()
        optimizer_state = self.optimizer.state_dict()
        # optimizer_state = self.train_op()
        
        # batch_log = rewards_log  ## add more recorded metrics in this log
        utils.add_prefix_to_dict_keys_inplace(
                rewards_log, prefix="SQL_ON/rewards/")
        
        batch_log = rewards_log
        batch_log['dpo_loss'] = loss

        return batch_log, optimizer_state
    
    def dpo_loss_calculation(self, step, batch, dpo_loss_config):

        

        policy_model = self.module.train()
        policy_model._pre_steps(step)  
        
        sum_reward, content_reward, style_reward, rewards_log, policy_all_logps, ref_all_logps = \
        self.sample_prompt_calculate_logp(batch, policy_model)
        
        sample_num = len(content_reward)
        
        
        idxs = list(range(sample_num))
        random.shuffle(idxs)
        idx_pairs = [(idxs[i], idxs[i+1]) for i in range(0, sample_num, 2)]
        idx_pairs = [(i, j) for i in range(sample_num) for j in range(i+1, sample_num)]
        
        chosen_idx_list = []
        rejected_idx_list = []
        non_dominate_list = [[], []]

        def log_extract(temp_list):
            policy_logs = policy_all_logps[temp_list]
            with torch.no_grad():
                ref_logs = ref_all_logps[temp_list]
            return policy_logs, ref_logs

        for idx1, idx2 in idx_pairs:
            c1 = content_reward[idx1]
            c2 = content_reward[idx2]
            s1, s2 = style_reward[idx1], style_reward[idx2]
            
            if c1>=c2 and s1>=s2 and (c1>c2 or s1>s2):
                chosen_idx_list.append(idx1)
                rejected_idx_list.append(idx2)
            elif c1<=c2 and s1<=s2 and (c1<c2 or s1<s2):
                chosen_idx_list.append(idx2)
                rejected_idx_list.append(idx1)
            else:
                non_dominate_list[0].append(idx1)
                non_dominate_list[1].append(idx2)
        losses1 = torch.tensor([]).to(policy_all_logps.device)
        losses2 = torch.tensor([]).to(policy_all_logps.device)
        
        if chosen_idx_list:
            policy_chosen_logps, reference_chosen_logps = log_extract(chosen_idx_list)
            policy_rejected_logps, reference_rejected_logps = log_extract(rejected_idx_list)
            losses1, chosen_rewards, rejected_rewards = preference_loss(
                policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps, self.dpo_loss_config)
        
        
        if dpo_loss_config.nondominate_punishment is None:
            pass
        elif dpo_loss_config.nondominate_punishment == 'prob_diff':
            
            if non_dominate_list:
                policy_log1, ref_log1 = log_extract(non_dominate_list[0])
                policy_log2, ref_log2 = log_extract(non_dominate_list[1])
                epsilon = self.dpo_loss_config.epsilon  # Epsilon value
                loss_fn = EpsilonInsensitiveLoss(epsilon)
                losses2 = loss_fn(policy_log1, policy_log2, ref_log1, ref_log2)
                
        losses = torch.concatenate((losses1, losses2))
        
        if len(losses) == 0:
            loss = torch.tensor(0.0, requires_grad=True)
        else:
            loss = losses.mean()
        
        return loss, rewards_log
    
    def sample_prompt_calculate_logp(self, batch, policy_model):
        (policy_logits, ref_logits, output_tokens, output_ids, sequence_lengths) = \
            policy_model._decode_sampling(batch=batch) # for multiobjective batch size should be large
        
        sample_num = policy_logits.shape[0]
        token_lenght = policy_logits.shape[1]
        
        policy_all_logps = policy_logits[
            torch.arange(sample_num).unsqueeze(1), 
            torch.arange(token_lenght), 
            output_ids
        ].sum(axis=1)
        
        ref_all_logps = ref_logits[
            torch.arange(sample_num).unsqueeze(1), 
            torch.arange(token_lenght), 
            output_ids
        ].sum(axis=1)
        
        #bypass the compute_rewards function: /hpcgpfs01/scratch/gzhao/rl-prompt/rlprompt/modules/sql_module.py
        sum_reward, content_reward, style_reward, rewards_log= \
            policy_model.compute_rewards(batch=batch, 
                                output_tokens=output_tokens,
                                mode="train", multi_optimize=self.dpo_loss_config.multi_optimize)
        
        return sum_reward, content_reward, style_reward, rewards_log, policy_all_logps, ref_all_logps

    
    def pareto_front(self, reward1, reward2):
        combined_rewards = torch.stack([reward1, reward2], dim=1)
    
        # Sort the combined rewards in descending order
        sorted_rewards, indices = torch.sort(combined_rewards, dim=0, descending=True)
        
        # Initialize the Pareto front and other layers indices
        pareto_front_indices = [indices[0, 0]]
        other_layers_indices = []
        
        # Iterate through the sorted rewards to identify the Pareto front and other layers
        for i in range(1, len(sorted_rewards)):
            if sorted_rewards[i, 1] < sorted_rewards[i-1, 1]:
                pareto_front_indices.append(indices[i, 0])
            else:
                other_layers_indices.append(indices[i, 0])
        
        return pareto_front_indices, other_layers_indices
    
    def find_dominate_indices(self, reward1, reward2):
        """
        Find the dominated indices from two reward tensors and return a dictionary.
        
        Parameters:
            reward1 (list or torch.Tensor): Reward tensor 1.
            reward2 (list or torch.Tensor): Reward tensor 2.
            
        Returns:
            dict: A dictionary where keys are indices of dominating elements and values are lists 
                of indices dominated by the corresponding key.
        """
        dominate_dict = {}
        for i, (r1, r2) in enumerate(zip(reward1, reward2)):
            for j, (rr1, rr2) in enumerate(zip(reward1, reward2)):
                if i != j:  # Avoid comparing an element with itself
                    if r1 >= rr1 and r2 >= rr2 and (r1 > rr1 or r2 > rr2):
                        # Element i dominates element j
                        dominate_dict.setdefault(i, []).append(j)
        return dominate_dict
        # self._target_model is the reference model
        #  outputs_ = self._target_model.teacher_forcing(**batch_) this one gives the logtis at: outputs_['sample_logits'].contiguous(),
            #    https://vscode.dev/github/mingkaid/rl-prompt/blob/main/rlprompt/modules/sql_module.py#L216
            
        # policy_chosen_logps = policy_logits[chosen_idx_list]
        # policy_rejected_logps = policy_logits[rejected_idx_list]
        # reference_chosen_logps = ref_logits[chosen_idx_list]
        # reference_rejected_logps = ref_logits[rejected_idx_list]
            
class EpsilonInsensitiveLoss(torch.nn.Module):
    def __init__(self, epsilon):
        super(EpsilonInsensitiveLoss, self).__init__()
        self.epsilon = epsilon

    def forward(self, policy_log1, policy_log2, ref_log1, ref_log2):
        logits = reward_diff(policy_log1, policy_log2, ref_log1, ref_log2)
        loss = torch.max(torch.abs(logits) - self.epsilon, torch.tensor(0.0))
        return loss
        